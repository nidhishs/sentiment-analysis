{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-10-16T20:39:38.948346Z","iopub.status.busy":"2021-10-16T20:39:38.948036Z","iopub.status.idle":"2021-10-16T20:39:43.688555Z","shell.execute_reply":"2021-10-16T20:39:43.687809Z","shell.execute_reply.started":"2021-10-16T20:39:38.948259Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","\n","from tensorflow import keras\n","from tensorflow.keras.layers import TextVectorization\n","from tensorflow.keras.layers import Embedding\n","from tensorflow.keras import layers"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-16T20:40:18.910097Z","iopub.status.busy":"2021-10-16T20:40:18.909427Z","iopub.status.idle":"2021-10-16T20:40:18.919324Z","shell.execute_reply":"2021-10-16T20:40:18.917804Z","shell.execute_reply.started":"2021-10-16T20:40:18.910062Z"},"trusted":true},"outputs":[],"source":["def standardize_text(input_text):\n","    emojis = {\n","    ':-*\\)': 'smile', ':-*]': 'smile', ':-*d': 'smile',\n","    ':-*\\(': 'frown', ':-*\\[': 'frown', ':-*/': 'unsure',\n","    ':-*o': 'astonish', ':-*0': 'astonish', 'xd': 'laugh',\n","    ';-*\\)': 'wink', \":'\\(\": 'cry', ':3': 'smile', '&lt;3': 'love',\n","    }\n","    # Convert to lower case\n","    input_text = tf.strings.lower(input_text)\n","\n","    # Remove all URLs, hashtags, mentions\n","    input_text = tf.strings.regex_replace(input_text, r'(https|http)?:\\/\\/\\S+', ' ')\n","    input_text = tf.strings.regex_replace(input_text, r'^#\\w+|\\s#\\w+', ' ')\n","    input_text = tf.strings.regex_replace(input_text, r'^@\\w+|\\s@\\w+', ' ')\n","\n","    # Convert all emojis to their text counterparts\n","    for emoji, emoji_text in emojis.items():\n","        input_text = tf.strings.regex_replace(input_text, emoji, emoji_text)\n","\n","    # Convert HTML references to text\n","    input_text = tf.strings.regex_replace(input_text, r'&amp;', 'and ')\n","    input_text = tf.strings.regex_replace(input_text, r'&quot;', '')\n","    input_text = tf.strings.regex_replace(input_text, r'&gt;', '')\n","\n","    # Remove non-ASCII characters\n","    input_text = tf.strings.regex_replace(input_text, r'\\w*[^\\x00-\\x7F]+\\w*', ' ')\n","\n","    # Remove additional spaces\n","    input_text = tf.strings.regex_replace(input_text, r'\\s\\s+', ' ')\n","    input_text = tf.strings.strip(input_text)\n","\n","\n","    return input_text"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-16T20:40:19.859117Z","iopub.status.busy":"2021-10-16T20:40:19.858859Z","iopub.status.idle":"2021-10-16T20:40:19.865777Z","shell.execute_reply":"2021-10-16T20:40:19.864996Z","shell.execute_reply.started":"2021-10-16T20:40:19.859087Z"},"trusted":true},"outputs":[],"source":["def word_lengthening(input_text):\n","    if len(input_text) < 1:\n","        return input_text\n","\n","    # Fix word-lengthening; convert Helllooooo to Helloo.\n","    word_count = 1\n","    input_text = input_text.lower()\n","    input_text_clean = [input_text[0]]\n","    for i in range(1, len(input_text)):\n","        if input_text[i] == input_text[i-1]:\n","            word_count += 1\n","        else:\n","            word_count = 1\n","        if word_count <= 2:\n","            input_text_clean.append(input_text[i])\n","        else:\n","            continue\n","    input_text = ''.join(input_text_clean)\n","    return input_text"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-16T20:40:33.809310Z","iopub.status.busy":"2021-10-16T20:40:33.809031Z","iopub.status.idle":"2021-10-16T20:40:33.817630Z","shell.execute_reply":"2021-10-16T20:40:33.816902Z","shell.execute_reply.started":"2021-10-16T20:40:33.809274Z"},"trusted":true},"outputs":[],"source":["def get_embeddings(vectorizer):\n","    glove_embedding_path = \"../input/glovetwitter27b100dtxt/glove.twitter.27B.200d.txt\"\n","    embeddings_index = {}\n","    with open(glove_embedding_path) as f:\n","        for line in f:\n","            word, coefs = line.split(maxsplit=1)\n","            coefs = np.fromstring(coefs, dtype=float, sep=\" \")\n","            embeddings_index[word] = coefs\n","\n","    print(\"Total word vectors: \", len(embeddings_index))\n","\n","    voc = vectorizer.get_vocabulary()\n","    word_index = dict(zip(voc, range(len(voc))))\n","\n","    num_tokens = len(voc) + 2\n","    embedding_dim = 200\n","    hits = 0\n","    misses = 0\n","    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n","    for word, i in word_index.items():\n","        embedding_vector = embeddings_index.get(word)\n","        if embedding_vector is not None:\n","            embedding_matrix[i] = embedding_vector\n","            hits += 1\n","        else:\n","            misses += 1\n","    print(f\"Converted {hits} words ({misses} misses).\")\n","\n","    return embedding_matrix, embedding_dim, num_tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-16T20:44:01.677413Z","iopub.status.busy":"2021-10-16T20:44:01.676636Z","iopub.status.idle":"2021-10-16T20:44:01.691541Z","shell.execute_reply":"2021-10-16T20:44:01.690779Z","shell.execute_reply.started":"2021-10-16T20:44:01.677360Z"},"trusted":true},"outputs":[],"source":["def build_model_cnn_biLSTM(embedding_matrix, embedding_dim, num_tokens, learning_rate=0.001):\n","    inp = keras.Input(shape=(None,), dtype=\"int64\")\n","    x = Embedding(\n","        num_tokens,\n","        embedding_dim,\n","        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n","        trainable=False,\n","    )(inp)\n","    x = layers.SpatialDropout1D(0.4)(x)\n","\n","    x_gru = layers.Bidirectional(layers.GRU(64, return_sequences=True))(x)\n","    x_gru_dr = layers.Dropout(0.4)(x_gru)\n","\n","    x1_conv1 = layers.Conv1D(128, 5, activation=\"relu\")(x_gru_dr)\n","    x1_conv1_maxpool = layers.MaxPooling1D(5)(x1_conv1)\n","    x1_conv2 = layers.Conv1D(64, 5, activation=\"relu\")(x1_conv1_maxpool)\n","    x1_avgpool = layers.GlobalAveragePooling1D()(x1_conv2)\n","    x1_maxpool = layers.GlobalMaxPool1D()(x1_conv2)\n","\n","    x_lstm = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n","    x_lstm_dr = layers.Dropout(0.4)(x_lstm)\n","\n","    x2_conv1 = layers.Conv1D(128, 5, activation=\"relu\")(x_lstm_dr)\n","    x2_conv1_maxpool = layers.MaxPooling1D(5)(x2_conv1)\n","    x2_conv2 = layers.Conv1D(64, 5, activation=\"relu\")(x2_conv1_maxpool)\n","    x2_avgpool = layers.GlobalAveragePooling1D()(x2_conv2)\n","    x2_maxpool = layers.GlobalMaxPool1D()(x2_conv2)\n","\n","\n","    x = keras.layers.concatenate(\n","        [x1_avgpool, x1_maxpool, x2_avgpool, x2_maxpool]\n","        )\n","    \n","    x = layers.BatchNormalization()(x)\n","    x = layers.Dense(64, activation='relu')(x)\n","    x = layers.Dropout(0.4)(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Dense(16, activation='relu')(x)\n","    x = layers.Dropout(0.4)(x)\n","    preds = layers.Dense(1, activation='sigmoid')(x)\n","\n","    model_cnn_biLSTM = keras.Model(inp, preds)\n","    model_cnn_biLSTM.summary()\n","\n","    model_cnn_biLSTM.compile(\n","        loss=\"binary_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), metrics=[\"acc\"]\n","    )\n","\n","    return model_cnn_biLSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-16T20:40:39.210573Z","iopub.status.busy":"2021-10-16T20:40:39.209983Z","iopub.status.idle":"2021-10-16T20:41:03.259073Z","shell.execute_reply":"2021-10-16T20:41:03.257815Z","shell.execute_reply.started":"2021-10-16T20:40:39.210534Z"},"trusted":true},"outputs":[],"source":["columns = ['target', 'ids', 'date', 'flag', 'user', 'text']\n","tweets = pd.read_csv('../training.1600000.processed.noemoticon.csv', encoding=\"latin-1\", names=columns)\n","tweets = tweets.sample(frac=0.6)\n","tweets.drop(columns=columns[1:5], inplace=True)\n","tweets.target.replace(4, 1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-16T20:44:18.128060Z","iopub.status.busy":"2021-10-16T20:44:18.127521Z","iopub.status.idle":"2021-10-16T20:47:42.534099Z","shell.execute_reply":"2021-10-16T20:47:42.533270Z","shell.execute_reply.started":"2021-10-16T20:44:18.128022Z"},"trusted":true},"outputs":[],"source":["tweets['clean_text'] = tweets.text.apply(word_lengthening)\n","vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=150, standardize=standardize_text)\n","vectorizer.adapt(tweets.clean_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-16T20:56:18.142024Z","iopub.status.busy":"2021-10-16T20:56:18.141417Z","iopub.status.idle":"2021-10-16T20:57:46.198846Z","shell.execute_reply":"2021-10-16T20:57:46.198147Z","shell.execute_reply.started":"2021-10-16T20:56:18.141984Z"},"trusted":true},"outputs":[],"source":["X_tweets = vectorizer(np.expand_dims(tweets.clean_text, axis=-1)).numpy()\n","y_tweets = np.array(tweets.target).reshape(-1, 1)\n","\n","embeddings_matrix, embedding_dim, num_tokens = get_embeddings(vectorizer)\n","fit_parameters = {'x': X_tweets, 'y': y_tweets, 'batch_size': 64, 'epochs': 20, 'validation_split': 0.05 }"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-16T20:54:24.856000Z","iopub.status.busy":"2021-10-16T20:54:24.855801Z","iopub.status.idle":"2021-10-16T20:54:26.020143Z","shell.execute_reply":"2021-10-16T20:54:26.019446Z","shell.execute_reply.started":"2021-10-16T20:54:24.855975Z"},"trusted":true},"outputs":[],"source":["model_cnn_biLSTM = build_model_cnn_biLSTM(embeddings_matrix, embedding_dim, num_tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-16T21:01:31.155339Z","iopub.status.busy":"2021-10-16T21:01:31.154464Z","iopub.status.idle":"2021-10-16T23:43:44.881015Z","shell.execute_reply":"2021-10-16T23:43:44.880260Z","shell.execute_reply.started":"2021-10-16T21:01:31.155213Z"},"trusted":true},"outputs":[],"source":["best_model_file_path = './model_cnn_biLSTM/best_model'\n","check_point = keras.callbacks.ModelCheckpoint(best_model_file_path, monitor = \"val_acc\", save_best_only = True, mode = \"max\")\n","early_stopping = keras.callbacks.EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n","history_cnn_biLSTM = model_cnn_biLSTM.fit(callbacks=[check_point, early_stopping], **fit_parameters)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat":4,"nbformat_minor":4}
